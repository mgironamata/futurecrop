{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-05T07:51:44.741485Z",
     "iopub.status.busy": "2025-09-05T07:51:44.740729Z",
     "iopub.status.idle": "2025-09-05T07:51:44.754957Z",
     "shell.execute_reply": "2025-09-05T07:51:44.753811Z",
     "shell.execute_reply.started": "2025-09-05T07:51:44.741456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/the-future-crop-challenge/pr_wheat_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmax_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/sample_submission.csv\n",
      "/kaggle/input/the-future-crop-challenge/soil_co2_wheat_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tas_wheat_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/rsds_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmin_wheat_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmax_wheat_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/rsds_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/soil_co2_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/train_solutions_maize.parquet\n",
      "/kaggle/input/the-future-crop-challenge/pr_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tas_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmax_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/pr_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/pr_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/soil_co2_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmin_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/train_solutions_wheat.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tas_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/soil_co2_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmin_maize_train.parquet\n",
      "/kaggle/input/the-future-crop-challenge/rsds_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmin_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tas_maize_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/tasmax_wheat_test.parquet\n",
      "/kaggle/input/the-future-crop-challenge/rsds_wheat_train.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/users/marron31/repos/futurecrop'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T07:51:47.283400Z",
     "iopub.status.busy": "2025-09-05T07:51:47.282462Z",
     "iopub.status.idle": "2025-09-05T07:51:47.288513Z",
     "shell.execute_reply": "2025-09-05T07:51:47.287395Z",
     "shell.execute_reply.started": "2025-09-05T07:51:47.283368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r'/users/marron31/repos/futurecrop'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:22:34.530900Z",
     "iopub.status.busy": "2025-09-05T08:22:34.530531Z",
     "iopub.status.idle": "2025-09-05T08:22:34.549748Z",
     "shell.execute_reply": "2025-09-05T08:22:34.548574Z",
     "shell.execute_reply.started": "2025-09-05T08:22:34.530875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# EALSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class EALSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Entity-Aware LSTM cell:\n",
    "      i_t = σ( W_xi x_t + W_hi h_{t-1} + W_si s + b_i )\n",
    "      f_t = σ( W_xf x_t + W_hf h_{t-1} + b_f )\n",
    "      o_t = σ( W_xo x_t + W_ho h_{t-1} + b_o )\n",
    "      g_t = tanh( W_xg x_t + W_hg h_{t-1} + b_g )\n",
    "      c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t\n",
    "      h_t = o_t ⊙ tanh(c_t)\n",
    "    Static features s only affect the input gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, static_size: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.static_size = static_size\n",
    "\n",
    "        # Dynamic projections (x_t and h_{t-1}) for all 4 gates at once\n",
    "        self.lin_x = nn.Linear(input_size, 4 * hidden_size, bias=True)\n",
    "        self.lin_h = nn.Linear(hidden_size, 4 * hidden_size, bias=True)\n",
    "\n",
    "        # Static projection ONLY into the input gate (no bias by convention)\n",
    "        self.lin_s = nn.Linear(static_size, hidden_size, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Xavier for weights; small biases (forget gate bias to ~1.0 is common)\n",
    "        for m in [self.lin_x, self.lin_h]:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.lin_s.weight)\n",
    "\n",
    "        # Set forget gate bias to positive (helps with long-term memory)\n",
    "        with torch.no_grad():\n",
    "            # gates layout: [i, f, o, g] in chunks of hidden_size\n",
    "            self.lin_x.bias[self.hidden_size:2*self.hidden_size].fill_(1.0)\n",
    "            self.lin_h.bias[self.hidden_size:2*self.hidden_size].fill_(0.0)\n",
    "\n",
    "    def forward(self, x_t: Tensor, s: Tensor, h_prev: Tensor, c_prev: Tensor):\n",
    "        \"\"\"\n",
    "        x_t: (B, input_size)\n",
    "        s:   (B, static_size)  -- same across all t for a given sample\n",
    "        h_prev, c_prev: (B, hidden_size)\n",
    "        returns: h_t, c_t\n",
    "        \"\"\"\n",
    "        gates = self.lin_x(x_t) + self.lin_h(h_prev)  # (B, 4H)\n",
    "\n",
    "        # Split gates\n",
    "        i, f, o, g = torch.chunk(gates, 4, dim=-1)\n",
    "\n",
    "        # Inject static features ONLY into input gate\n",
    "        i = i + self.lin_s(s)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c_t = f * c_prev + i * g\n",
    "        h_t = o * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "class EALSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-layer EA-LSTM unrolled over time.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, static_size: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = EALSTMCell(input_size, hidden_size, static_size)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout and dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x_seq: Tensor, x_static: Tensor, h0: Tensor = None, c0: Tensor = None, return_sequences: bool = False):\n",
    "        \"\"\"\n",
    "        x_seq:    (B, T, F) dynamic inputs\n",
    "        x_static: (B, S)    static features\n",
    "        h0, c0:   optional initial states, shape (B, H)\n",
    "        \"\"\"\n",
    "        B, T, _ = x_seq.shape\n",
    "        H = self.hidden_size\n",
    "        device = x_seq.device\n",
    "\n",
    "        if h0 is None:\n",
    "            h_t = torch.zeros(B, H, device=device, dtype=x_seq.dtype)\n",
    "        else:\n",
    "            h_t = h0\n",
    "\n",
    "        if c0 is None:\n",
    "            c_t = torch.zeros(B, H, device=device, dtype=x_seq.dtype)\n",
    "        else:\n",
    "            c_t = c0\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            x_t = x_seq[:, t, :]\n",
    "            h_t, c_t = self.cell(x_t, x_static, h_t, c_t)\n",
    "            h_t = self.dropout(h_t)\n",
    "            if return_sequences:\n",
    "                outputs.append(h_t)\n",
    "\n",
    "        if return_sequences:\n",
    "            # (B, T, H)\n",
    "            h_seq = torch.stack(outputs, dim=1)\n",
    "            return h_seq, (h_t, c_t)\n",
    "        else:\n",
    "            # Last hidden state (B, H)\n",
    "            return h_t, (h_t, c_t)\n",
    "\n",
    "\n",
    "class YieldNetEALSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Yield predictor using an EA-LSTM over daily climate inputs, conditioned on static soil/site features.\n",
    "    By default, predicts from the final hidden state. Optionally concatenate static features in the head.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_input_size: int,     # F\n",
    "        static_size: int,        # S\n",
    "        hidden_size: int,        # H\n",
    "        head_layers: list[int] = None,  # e.g., [H, 64, 1] or [H+S, 64, 1] if concat_static_in_head=True\n",
    "        dropout: float = 0.0,\n",
    "        concat_static_in_head: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.concat_static_in_head = concat_static_in_head\n",
    "        self.ealstm = EALSTM(seq_input_size, hidden_size, static_size, dropout=dropout)\n",
    "\n",
    "        if head_layers is None:\n",
    "            in_size = hidden_size + (static_size if concat_static_in_head else 0)\n",
    "            head_layers = [in_size, 1]\n",
    "\n",
    "        linears = []\n",
    "        for i in range(len(head_layers) - 1):\n",
    "            linears.append(nn.Linear(head_layers[i], head_layers[i+1]))\n",
    "            if i < len(head_layers) - 2:\n",
    "                linears.append(nn.ReLU())\n",
    "                if dropout and dropout > 0:\n",
    "                    linears.append(nn.Dropout(dropout))\n",
    "        self.head = nn.Sequential(*linears)\n",
    "\n",
    "        # Init for head\n",
    "        for m in self.head:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_seq: Tensor, x_static: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x_seq:    (B, T, F)\n",
    "        x_static: (B, S)\n",
    "        returns:  (B,) yield prediction\n",
    "        \"\"\"\n",
    "        h_last, _ = self.ealstm(x_seq, x_static, return_sequences=False)\n",
    "        if self.concat_static_in_head:\n",
    "            z = torch.cat([h_last, x_static], dim=-1)\n",
    "        else:\n",
    "            z = h_last\n",
    "        y = self.head(z).squeeze(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:22:37.027571Z",
     "iopub.status.busy": "2025-09-05T08:22:37.026681Z",
     "iopub.status.idle": "2025-09-05T08:22:37.037371Z",
     "shell.execute_reply": "2025-09-05T08:22:37.036225Z",
     "shell.execute_reply.started": "2025-09-05T08:22:37.027538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# YieldNet\n",
    "class YieldNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines yield neural network. A LSTM processes the daily climate feature timeseries, while a parallel branch \n",
    "    handles fixed soil properties per location and year in a fully connected network (FCN). \n",
    "    The resuls of both branches are merged to a single yield prediction (again, per location and year).\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_input_size, seq_hidden_size, fcn_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_input_size = seq_input_size\n",
    "        self.seq_hidden_size = seq_hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=seq_input_size, hidden_size=seq_hidden_size, num_layers=1, batch_first=True)\n",
    "        self.seq_fcn = nn.Linear(self.seq_hidden_size, 1)\n",
    "\n",
    "        self.fcn_layers = fcn_layers\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linears = nn.ModuleList([nn.Linear(fcn_layers[i], fcn_layers[i+1]) for i in range(len(fcn_layers)-1)]) \n",
    "\n",
    "        for i in range(len(fcn_layers)-1):         \n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)            \n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "    \n",
    "    def forward(self, x_seq, x_fcn):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, x_seq.shape[0], self.seq_hidden_size)).to(x_seq.device)\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, x_seq.shape[0], self.seq_hidden_size)).to(x_seq.device)\n",
    "\n",
    "        out, _ = self.lstm(x_seq, (h0, c0))\n",
    "        out = self.seq_fcn(out[:, -1, :])\n",
    "\n",
    "        a = x_fcn\n",
    "        for i in range(len(self.fcn_layers) - 2):  \n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linears[-1](a)\n",
    "\n",
    "        return out.reshape(out.shape[0], out.shape[1]) * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:03.478926Z",
     "iopub.status.busy": "2025-09-05T08:25:03.478578Z",
     "iopub.status.idle": "2025-09-05T08:25:03.497526Z",
     "shell.execute_reply": "2025-09-05T08:25:03.496442Z",
     "shell.execute_reply.started": "2025-09-05T08:25:03.478897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClimateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The ClimateDataset class provides a convenient way for acessing, merging and scaling parquet data. \n",
    "    This is used in the main training loop to access features and target variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, crop: str, mode: str, data_dir: str, scalers: list = None):\n",
    "        self.tasmax = pd.read_parquet(os.path.join(data_dir, f\"tasmax_{crop}_{mode}.parquet\"))\n",
    "        self.tasmin = pd.read_parquet(os.path.join(data_dir, f\"tasmin_{crop}_{mode}.parquet\"))\n",
    "        # self.tas = pd.read_parquet(os.path.join(data_dir, f\"tas_{crop}_{mode}.parquet\"))\n",
    "        self.pr = pd.read_parquet(os.path.join(data_dir, f\"pr_{crop}_{mode}.parquet\"))\n",
    "        self.rsds = pd.read_parquet(os.path.join(data_dir, f\"rsds_{crop}_{mode}.parquet\"))\n",
    "        self.soil_co2 = pd.read_parquet(os.path.join(data_dir, f\"soil_co2_{crop}_{mode}.parquet\"))\n",
    "\n",
    "        self.soil_co2['lon_sin'] = np.sin(self.soil_co2['lon'])*2*math.pi/365\n",
    "        self.soil_co2['lon_cos'] = np.cos(self.soil_co2['lon'])*2*math.pi/365\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.yield_ = pd.read_parquet(os.path.join(data_dir, f\"{mode}_solutions_{crop}.parquet\"))\n",
    "        else:\n",
    "            self.yield_ = None\n",
    "\n",
    "        self.locs = self.tasmax[['lat','lon']].copy()\n",
    "        # self.locs['ID'] = self.locs.index.copy()\n",
    "\n",
    "        self.yield_loc = pd.merge(self.yield_, self.locs, on='ID')\n",
    "        self.yield_var = self.yield_loc.groupby(['lon','lat'])['yield'].var()\n",
    "        self.yield_loc_with_var = pd.merge(self.yield_loc, self.yield_var, on=['lon','lat'], suffixes=['','_var'])\n",
    "        # self.yield_loc_with_var['yield_var'] = self.yield_loc_with_var['yield_var'].transform(lambda x: max(x, 0.1))\n",
    "        # self.yield_loc_with_var['yield_var'] = self.yield_loc_with_var['yield_var'].transform(lambda x: (x - x.min()) / (x.max() - x.min()) + 1) \n",
    "        # self.yield_loc_with_var['log_yield'] = self.yield_loc_with_var['yield'].transform(lambda x: np.log(x+1))\n",
    "        \n",
    "        if scalers is None:\n",
    "            self._init_scalers()\n",
    "        else:\n",
    "            self.scaler_climate, self.scaler_soil, self.scaler_yield = scalers\n",
    "        self._check_data([self.tasmax, self.tasmin, self.pr, self.rsds, self.soil_co2], self.yield_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 240x4 climate matrix per location/year (features in last dimension by convention)\n",
    "        climate = np.vstack([\n",
    "            self.tasmax.iloc[index, 5:].astype(np.float32), \n",
    "            self.tasmin.iloc[index, 5:].astype(np.float32),\n",
    "            self.pr.iloc[index, 5:].astype(np.float32),\n",
    "            self.rsds.iloc[index, 5:].astype(np.float32),\n",
    "        ]).T\n",
    "\n",
    "        # Fixed soil properties per location/year\n",
    "        soil = self.soil_co2.iloc[index][['co2', 'nitrogen','lat','lon_cos','lon_sin']].astype(np.float32)\n",
    "        \n",
    "        id = soil.name\n",
    "        soil = soil.values\n",
    "\n",
    "        # soil = np.concatenate([soil, loc_enc], axis=0)\n",
    "\n",
    "        # Yield estimated by process model\n",
    "        if self.yield_ is not None:\n",
    "\n",
    "            # retrieving log yield\n",
    "            yield_ = self.yield_loc_with_var.iloc[index,0].astype(np.float32)\n",
    "            yield_ = self.scaler_yield.transform(yield_.reshape(1, -1)).reshape(-1)\n",
    "\n",
    "            # yieldvar_ = self.yield_loc_with_var.iloc[index,-1].astype(np.float32)\n",
    "            \n",
    "        else:\n",
    "            yield_ = None\n",
    "            yieldvar_ = None\n",
    "\n",
    "        climate = self.scaler_climate.transform(climate)\n",
    "        soil = self.scaler_soil.transform(soil.reshape(1, -1)).reshape(-1)\n",
    "\n",
    "        return torch.tensor(climate), torch.tensor(soil), torch.tensor(yield_ or []), id\n",
    "        # return torch.tensor(climate), torch.tensor(soil), torch.tensor(yield_ or []), id, torch.tensor(yieldvar_ or [])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tasmax.shape[0]\n",
    "        # return 1000\n",
    "\n",
    "    def _init_scalers(self):\n",
    "        # Draw random sample from climate data to estimate distribution moments for scaler.\n",
    "        climate_sample = np.vstack([\n",
    "            self.tasmax.sample(1000).iloc[:, 5:].values.flatten(), \n",
    "            self.tasmin.sample(1000).iloc[:, 5:].values.flatten(),\n",
    "            self.pr.sample(1000).iloc[:, 5:].values.flatten(),\n",
    "            self.rsds.sample(1000).iloc[:, 5:].values.flatten(),\n",
    "        ]).T\n",
    "        self.scaler_climate = StandardScaler()\n",
    "        self.scaler_climate.fit(climate_sample)\n",
    "\n",
    "        # Scaler for fixed soil properties\n",
    "        self.scaler_soil = StandardScaler()\n",
    "        self.scaler_soil.fit(self.soil_co2[['co2', 'nitrogen','lat','lon_cos','lon_sin']].values)\n",
    "\n",
    "        # Scaler for yield\n",
    "        self.scaler_yield = StandardScaler()\n",
    "        if self.yield_ is not None:\n",
    "            self.scaler_yield.fit(self.yield_.values)\n",
    "\n",
    "    def _check_data(self, climate: list, yield_: pd.DataFrame) -> bool:\n",
    "        # Check for matching year, lon, lat columns\n",
    "        for i in range(1, len(climate)):\n",
    "            assert np.all(climate[0][['year', 'lon', 'lat']] == climate[i][['year', 'lon', 'lat']])\n",
    "        # Check label for matching length\n",
    "        assert yield_ is None or climate[0].shape[0] == yield_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:24:06.665104Z",
     "iopub.status.busy": "2025-09-05T08:24:06.664480Z",
     "iopub.status.idle": "2025-09-05T08:24:12.225558Z",
     "shell.execute_reply": "2025-09-05T08:24:12.224597Z",
     "shell.execute_reply.started": "2025-09-05T08:24:06.665067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Intialize model, data loader, loss and optimizer\n",
    "ds_train = ClimateDataset('maize', 'train', data_dir=DATA_DIR)\n",
    "train_loader = DataLoader(ds_train, batch_size=64, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:07.819095Z",
     "iopub.status.busy": "2025-09-05T08:25:07.818322Z",
     "iopub.status.idle": "2025-09-05T08:25:07.834568Z",
     "shell.execute_reply": "2025-09-05T08:25:07.833580Z",
     "shell.execute_reply.started": "2025-09-05T08:25:07.819065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 128, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_feat = ds_train[1][0].shape[-1]   # e.g., 4\n",
    "H = 128\n",
    "S = ds_train[1][1].shape[-1]   \n",
    "\n",
    "S, H, seq_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:09.787654Z",
     "iopub.status.busy": "2025-09-05T08:25:09.786955Z",
     "iopub.status.idle": "2025-09-05T08:25:09.798941Z",
     "shell.execute_reply": "2025-09-05T08:25:09.797973Z",
     "shell.execute_reply.started": "2025-09-05T08:25:09.787627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = YieldNetEALSTM(\n",
    "    seq_input_size=seq_feat,\n",
    "    static_size=S,\n",
    "    hidden_size=H,\n",
    "    head_layers=[H + S, 64, 1],  # <— 130 here\n",
    "    dropout=0.1,\n",
    "    concat_static_in_head=True\n",
    ").to(device)\n",
    "\n",
    "# model = YieldNet(seq_input_size=4, seq_hidden_size=128, fcn_layers=[S, 32, 32, 1]).to(device)\n",
    "\n",
    "cost_mse = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def model_device(m):\n",
    "    return next(m.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:10.169387Z",
     "iopub.status.busy": "2025-09-05T08:25:10.168768Z",
     "iopub.status.idle": "2025-09-05T08:25:10.173842Z",
     "shell.execute_reply": "2025-09-05T08:25:10.173071Z",
     "shell.execute_reply.started": "2025-09-05T08:25:10.169358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "load_checkpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "\n",
    "    checkpoint = torch.load(\"/kaggle/working/checkpoints/lstm_location/epoch_3.pt\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:10.645102Z",
     "iopub.status.busy": "2025-09-05T08:25:10.644350Z",
     "iopub.status.idle": "2025-09-05T08:25:10.649180Z",
     "shell.execute_reply": "2025-09-05T08:25:10.648192Z",
     "shell.execute_reply.started": "2025-09-05T08:25:10.645073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:12.804571Z",
     "iopub.status.busy": "2025-09-05T08:25:12.803769Z",
     "iopub.status.idle": "2025-09-05T08:25:12.810449Z",
     "shell.execute_reply": "2025-09-05T08:25:12.809476Z",
     "shell.execute_reply.started": "2025-09-05T08:25:12.804539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77889"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T08:25:13.021740Z",
     "iopub.status.busy": "2025-09-05T08:25:13.020962Z",
     "iopub.status.idle": "2025-09-05T08:25:13.033343Z",
     "shell.execute_reply": "2025-09-05T08:25:13.032306Z",
     "shell.execute_reply.started": "2025-09-05T08:25:13.021708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.5227, -0.8337,  0.4713, -0.5730, -1.2823]),\n",
       " tensor([-1.0632]),\n",
       " 3000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task[0].shape, task[1], task[2], task[3]\n",
    "ds_train[3000][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-05T09:40:54.776Z",
     "iopub.execute_input": "2025-09-05T08:25:13.477402Z",
     "iopub.status.busy": "2025-09-05T08:25:13.476966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2a326b36ba404ab31119065538982e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- loss: 1.0978884697\n",
      "100 -- loss: 1.0220752506\n",
      "200 -- loss: 1.0161526310\n",
      "300 -- loss: 1.0103548977\n",
      "400 -- loss: 1.0029603708\n",
      "500 -- loss: 1.0031274621\n",
      "600 -- loss: 1.0019174413\n",
      "700 -- loss: 0.9965851357\n",
      "800 -- loss: 0.9985041705\n",
      "900 -- loss: 0.9987341731\n",
      "1000 -- loss: 0.9993326206\n",
      "1100 -- loss: 0.9986532334\n",
      "1200 -- loss: 0.9987741121\n",
      "1300 -- loss: 0.9990034068\n",
      "1400 -- loss: 0.9974697124\n",
      "1500 -- loss: 0.9996482654\n",
      "1600 -- loss: 0.9986375407\n",
      "1700 -- loss: 0.9987637310\n",
      "1800 -- loss: 0.9994348737\n",
      "1900 -- loss: 1.0004388048\n",
      "2000 -- loss: 0.9999675373\n",
      "2100 -- loss: 0.9985731284\n",
      "2200 -- loss: 0.9995767428\n",
      "2300 -- loss: 0.9994997172\n",
      "2400 -- loss: 0.9993758010\n",
      "2500 -- loss: 0.9989218766\n",
      "2600 -- loss: 0.9979687307\n",
      "2700 -- loss: 0.9972301224\n",
      "2800 -- loss: 0.9975232727\n",
      "2900 -- loss: 0.9965779131\n",
      "3000 -- loss: 0.9976155941\n",
      "3100 -- loss: 0.9978519903\n",
      "3200 -- loss: 0.9986435981\n",
      "3300 -- loss: 0.9980051999\n",
      "3400 -- loss: 0.9980620271\n",
      "3500 -- loss: 0.9993258882\n",
      "3600 -- loss: 0.9991237778\n",
      "3700 -- loss: 0.9991643606\n",
      "3800 -- loss: 0.9993185093\n",
      "3900 -- loss: 0.9988405827\n",
      "4000 -- loss: 0.9988118130\n",
      "4100 -- loss: 0.9997011960\n",
      "4200 -- loss: 0.9999447333\n",
      "4300 -- loss: 0.9997776341\n",
      "4400 -- loss: 1.0004630466\n",
      "4500 -- loss: 1.0000341117\n",
      "4600 -- loss: 1.0002408456\n",
      "4700 -- loss: 1.0002187389\n",
      "4800 -- loss: 1.0005992691\n",
      "4900 -- loss: 1.0005284487\n",
      "5000 -- loss: 1.0004996727\n"
     ]
    }
   ],
   "source": [
    "# Directory to save checkpoints\n",
    "ckpt_dir = \"./checkpoints/ealstm_location\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# Main training loop\n",
    "# In this example the model is only trained on maize data!\n",
    "epoch_loss = []\n",
    "num_epochs = 3\n",
    "# pbar = tqdm(range(num_epochs))\n",
    "pbar = range(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    loss = []\n",
    "    model.train()\n",
    "    for i, (climate, soil, yield_true, _) in enumerate(tqdm(train_loader, leave=False)):\n",
    "        dev = model_device(model)\n",
    "        optimizer.zero_grad()\n",
    "        yield_pred = model(climate.to(device), soil.to(device))\n",
    "        data_loss = cost_mse(yield_pred, yield_true.to(device))\n",
    "        # data_loss = ((yield_pred - yield_true.to(device))**2 / yield_var.to(device)).mean()\n",
    "        data_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional but helpful\n",
    "        optimizer.step()\n",
    "\n",
    "        loss.append(data_loss.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{i} -- loss: {np.mean(loss):5.10f}\")\n",
    "\n",
    "    epoch_loss.append(np.mean(loss))\n",
    "    print(f\"Epoch {epoch} - loss: {np.mean(loss):5.10f}\")\n",
    "    print(f\"Elapsed: {(time.time() - start):5.2f}\")\n",
    "\n",
    "    # --- Save checkpoint ---\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch+1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': np.mean(loss),\n",
    "    }, ckpt_path)\n",
    "    print(f\"Checkpoint saved: {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Predict test set for submission\n",
    "ds_test_maize = ClimateDataset('maize', 'test', data_dir=DATA_DIR, scalers=(ds_train.scaler_climate, ds_train.scaler_soil, ds_train.scaler_yield))\n",
    "# ds_test_wheat = ClimateDataset('wheat', 'test', data_dir=DATA_DIR, scalers=(ds_train.scaler_climate, ds_train.scaler_soil, ds_train.scaler_yield))\n",
    "\n",
    "test_loader_maize = DataLoader(ds_test_maize, batch_size=400, shuffle=False, num_workers=6)\n",
    "# test_loader_wheat = DataLoader(ds_test_wheat, batch_size=100, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ids = []\n",
    "yields_pred = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, (climate, soil, _, id) in enumerate(tqdm(test_loader_maize)):\n",
    "    start = time.time()\n",
    "    ids.append(id.detach().numpy())\n",
    "    yields_pred.append(model(climate.to(device), soil.to(device)).detach().cpu().numpy())\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print(f\"Time elapsed: {time.time() - start}\")\n",
    "\n",
    "# for i, (climate, soil, _, id) in enumerate(tqdm(test_loader_wheat)):\n",
    "#     ids.append(id.detach().numpy())\n",
    "#     yields_pred.append(model(climate.to(device), soil.to(device)).detach().cpu().numpy())\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Concatenate all batches\n",
    "# ids = np.concatenate(ids_list, axis=0)\n",
    "# yields_pred = np.concatenate(yields_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yields_pred = np.concatenate(yields_pred)\n",
    "yields_pred = ds_train.scaler_yield.inverse_transform(yields_pred)\n",
    "yields_pred = yields_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(yields_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = pd.Series(yields_pred, index=np.concatenate(ids))\n",
    "predictions.index.name = 'ID'\n",
    "predictions.name = 'yield'\n",
    "predictions.to_csv('/kaggle/working/last_loc_6ep_maize_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8812083,
     "sourceId": 81000,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
